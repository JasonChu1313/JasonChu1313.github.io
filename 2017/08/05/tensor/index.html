<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>TensorFlow Tutorial-1 | 朱思宇的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1、Why TensorFlow?网上有关介绍太多了，我就不多说了，这里主要注重使用。   2、Programing model2.1、Big Idea：将数值的计算转化为图（computational graph），任何tensorflow的计算都是基于图的。 2.2、Graph Nodes：图中的结点是有输入和输出的操作（operations），input的数量可以任意，output只有一个。">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow Tutorial-1">
<meta property="og:url" content="http://yoursite.com/2017/08/05/tensor/index.html">
<meta property="og:site_name" content="朱思宇的博客">
<meta property="og:description" content="1、Why TensorFlow?网上有关介绍太多了，我就不多说了，这里主要注重使用。   2、Programing model2.1、Big Idea：将数值的计算转化为图（computational graph），任何tensorflow的计算都是基于图的。 2.2、Graph Nodes：图中的结点是有输入和输出的操作（operations），input的数量可以任意，output只有一个。">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-5fecb3836a99ad46.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-bfb78a2ee08d6c22.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-f75959626599e7ad.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-5d1114cd67b83f36.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-79d6240b234f8219.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="http://upload-images.jianshu.io/upload_images/7058214-44a150cc63f80034.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2017-10-03T15:32:57.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow Tutorial-1">
<meta name="twitter:description" content="1、Why TensorFlow?网上有关介绍太多了，我就不多说了，这里主要注重使用。   2、Programing model2.1、Big Idea：将数值的计算转化为图（computational graph），任何tensorflow的计算都是基于图的。 2.2、Graph Nodes：图中的结点是有输入和输出的操作（operations），input的数量可以任意，output只有一个。">
<meta name="twitter:image" content="http://upload-images.jianshu.io/upload_images/7058214-5fecb3836a99ad46.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
  
    <link rel="alternate" href="/atom.xml" title="朱思宇的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">朱思宇的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-tensor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/05/tensor/" class="article-date">
  <time datetime="2017-08-05T11:59:00.000Z" itemprop="datePublished">2017-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TensorFlow Tutorial-1
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1、Why-TensorFlow"><a href="#1、Why-TensorFlow" class="headerlink" title="1、Why TensorFlow?"></a>1、Why TensorFlow?</h1><p>网上有关介绍太多了，我就不多说了，这里主要注重使用。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-5fecb3836a99ad46.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Intro.PNG"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-bfb78a2ee08d6c22.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="github.PNG"></p>
<h1 id="2、Programing-model"><a href="#2、Programing-model" class="headerlink" title="2、Programing model"></a>2、Programing model</h1><h2 id="2-1、Big-Idea："><a href="#2-1、Big-Idea：" class="headerlink" title="2.1、Big Idea："></a>2.1、Big Idea：</h2><p>将数值的计算转化为图（computational graph），任何tensorflow的计算都是基于图的。</p>
<h2 id="2-2、Graph-Nodes："><a href="#2-2、Graph-Nodes：" class="headerlink" title="2.2、Graph Nodes："></a>2.2、Graph Nodes：</h2><p>图中的结点是有输入和输出的操作（operations），input的数量可以任意，output只有一个。</p>
<h2 id="2-3、Graph-Edges："><a href="#2-3、Graph-Edges：" class="headerlink" title="2.3、Graph Edges："></a>2.3、Graph Edges：</h2><p> 图的边是在结点见浮动的张量（tensors），tensors可以理解为n-维数组。</p>
<h2 id="2-4、Advantages："><a href="#2-4、Advantages：" class="headerlink" title="2.4、Advantages："></a>2.4、Advantages：</h2><p>使用flow graphs作为deep learning framework的优点是可以通过简单少量的operations来构建复杂模型，使梯度的计算容易很多。当你在编写比较大的模型时，自动微分将会帮助到你。</p>
<h2 id="2-5、Another-Way-To-Think-It："><a href="#2-5、Another-Way-To-Think-It：" class="headerlink" title="2.5、Another Way To Think It："></a>2.5、Another Way To Think It：</h2><p>每一个operation可以理解为在某一点可以执行的函数。</p>
<h2 id="2-6、举个详细使用的栗子："><a href="#2-6、举个详细使用的栗子：" class="headerlink" title="2.6、举个详细使用的栗子："></a>2.6、举个详细使用的栗子：</h2><p>下面这个图展示了只有一个隐藏层的神经网络在Tensorflow中的计算，我们以Relu作为激活函数。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-f75959626599e7ad.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Nodes.PNG"></p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-5d1114cd67b83f36.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ReLu Function.PNG"></p>
<ul>
<li>W：  待训练的参数</li>
<li>X：   input</li>
<li>b：   bias<br>下面将介绍一下上图中出现的几个结点：<h3 id="1、Variables："><a href="#1、Variables：" class="headerlink" title="1、Variables："></a>1、Variables：</h3>W 和 b 参数作为TensorFlow中的Variables，在训练的过程中你需要调整这些参数，使得你的loss function最小，这些变量是有状态的结点，在图中多种的计算结点之间保持他们的状态，所谓保持状态就是指它们的值会被存下来，因此想要复原数据很容易，并且可以随时输出他们当前的值（current value），这些变量还有一些其他的features，可以在训练或者训练结束后持久化到disk中，因此可以允许不同的公司不同的组织去使用这些大型模型训练好的参数，并且默认进行梯度更新。<br>W 和 b这些变量也是operations。</li>
</ul>
<h3 id="2、Placeholders："><a href="#2、Placeholders：" class="headerlink" title="2、Placeholders："></a>2、Placeholders：</h3><p>是一些在执行的过程中才会被赋值的结点，如果你网络的input 需要依赖一些其他外界的数据。<br>比如你不想用真实的值来计算。placeholders是你在训练过程中可以加入数据的地方。对于placeholders，我们不用对其进行任何初始化，我们只定义一个data type，并且赋值一个给定大小的tensor，我们的计算图就可以知道怎么去计算，甚至不用存储任何的数据。</p>
<h3 id="3、Mathematical-operations："><a href="#3、Mathematical-operations：" class="headerlink" title="3、Mathematical operations："></a>3、Mathematical operations：</h3><p>用于矩阵的乘法，加法，ReLu函数的计算。</p>
<h3 id="4、废话少说，上代码！！！-："><a href="#4、废话少说，上代码！！！-：" class="headerlink" title="4、废话少说，上代码！！！ ："></a>4、废话少说，上代码！！！ ：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># 导入tensorflow包</div><div class="line">import tensorflow as tf</div><div class="line"># 创建一个有100个值的vector，作为bias, 默认为0</div><div class="line">b=tf.Variable(tf.zeros((100,)))</div><div class="line"># 创建并出示化权重，一个784*100 的矩阵，矩阵的初始值在-1到1之间</div><div class="line">W=tf.Variable(tf.random_uniform((784,100),-1,1))</div><div class="line">#为输入创建一个placeholder，不需要任何数据，只需要定义数据类型为一个32位浮点数，shape 为100*784</div><div class="line">x=tf.placeholder(tf.float32,(100,784))</div><div class="line"># tensorflow mathematical operations</div><div class="line">h=tf.nn.relu(tf.matmul(x,W)+b)</div></pre></td></tr></table></figure>
<p>关于h我想说：和numpy中很像，就是调用了tensorflow mathematical operations，我们没有真的乘某个数值，而仅仅在图中创建了一个符号来表示他，所以你不能输出该值，因为x只是一个placeholder，没有任何真值。我们现在只为我们的model创建了一个骨架。</p>
<h3 id="5、说好的图呢-："><a href="#5、说好的图呢-：" class="headerlink" title="5、说好的图呢 ："></a>5、说好的图呢 ：</h3><p>看了半天，这不和numpy手撸一样吗，其实作为程序员我们只要心里有着种抽象的概念就好，底层确实是以这种结点来实现的，如果你想看到，可以调用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tf.get_default_graph().get_operations()</div></pre></td></tr></table></figure></p>
<p>你将会看到如下内容：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-79d6240b234f8219.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Absta.PNG"></p>
<h3 id="6、怎么跑这个例子呢？-："><a href="#6、怎么跑这个例子呢？-：" class="headerlink" title="6、怎么跑这个例子呢？ ："></a>6、怎么跑这个例子呢？ ：</h3><p>目前为止，我们已经定义了一个graph，我们需要把这个graph部署到一个session中，一个session可以绑定到一个具体的执行环境中（CPU或者GPU）</p>
<p>接着刚才的代码，我们在后面补充三行：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import tensorflow as tf</div><div class="line">b=tf.Variable(tf.zeros((100,)))</div><div class="line">W=tf.Variable(tf.random_uniform((784,100),-1,1))</div><div class="line">x=tf.placeholder(tf.float32,(100,784))</div><div class="line">h=tf.nn.relu(tf.matmul(x,W)+b)</div><div class="line"></div><div class="line">#创建session对象，初始化相关的参数</div><div class="line">sess=tf.Session()</div><div class="line"># initialize b 和 w</div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line"># 第一个参数是图中结点的输出，第二个参数是给placeholder赋的值，是一个map，定义每个结点的具体值</div><div class="line">sess.run(h,&#123;x : np.random.random(100,784)&#125;)</div></pre></td></tr></table></figure>
<p>更多的关于Session的使用方法:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">#自己创建一个session,不使用默认的session,使用完记得关了</div><div class="line">sess=tf.Session()</div><div class="line">sess=tf.run(train_step)</div><div class="line">sess.close()</div><div class="line"></div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    #使用这个创建的session计算结果</div><div class="line">    sess.run(train_step)</div><div class="line">    #不用close,with体帮我们进行资源的回收</div><div class="line"></div><div class="line">#with体使用默认的session</div><div class="line">sess=tf.Session()</div><div class="line">with sess.as_default():</div><div class="line">    print(train_step.eval())</div><div class="line"></div><div class="line"></div><div class="line">#也是使用默认的session</div><div class="line">sess=tf.Session()</div><div class="line">print(train_step.eval(session=sess))</div><div class="line"></div><div class="line"></div><div class="line">#定义一个可交互式的session,自动将会话注册为默认会话</div><div class="line">sess=tf.InteractiveSession()</div><div class="line">train_step.eval()</div><div class="line">sess.close()</div><div class="line"></div><div class="line"></div><div class="line">#用自己的参数配置session</div><div class="line">config=tf.ConfigProto(allow_aoft_placement=True,</div><div class="line">                      log_device_placement=True)</div><div class="line">sess1=tf.InteractiveSession(config=config)</div><div class="line">sess2=tf.Session(config=config)</div></pre></td></tr></table></figure></p>
<h3 id="7、损失函数的创建-："><a href="#7、损失函数的创建-：" class="headerlink" title="7、损失函数的创建 ："></a>7、损失函数的创建 ：</h3><p>通过prediction 和 labels 创建loss node<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># neural network的最后一步，用softmax做一个逻辑回归</div><div class="line">prediction=tf.nn.softmax(...)</div><div class="line">label=tf.placeholder(tf.float32,[100,10])</div><div class="line"># 损失函数，用label乘上logP在列上的值</div><div class="line">cross_entropy=-tf.reduce_sum(label* tf.log(prediction),axis=1)</div></pre></td></tr></table></figure></p>
<h3 id="8、如何计算梯度？："><a href="#8、如何计算梯度？：" class="headerlink" title="8、如何计算梯度？："></a>8、如何计算梯度？：</h3><p>tf.train.GradientDescentOptimizer创建了一个Optimizer是TensorFlow中定义的一个抽象类，它的每个子类，可以作为一个特定的学习算法，默认是梯度下降。在我们的图上面加了一个optimization operation，当我们执行这个train_step方法时（sess.run(train_step,feed_dict={x: batch_x, label: batch_label})）,将会应用所有的梯度到模型中的变量中。这是因为minimize函数做了两件事情，首先是计算cross_entropy的梯度，然后进行梯度更新。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 梯度下降，learning rate 为0.5，minimize方法的参数是一个需要被梯度下降的结点。</div><div class="line">train_step= tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</div></pre></td></tr></table></figure>
<h3 id="8、训练模型？："><a href="#8、训练模型？：" class="headerlink" title="8、训练模型？："></a>8、训练模型？：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">sess=tf.Session()</div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line"># 创建一个learning schedule, iterate 1000 次</div><div class="line">for i in range(1000):</div><div class="line">        batch_x, batch_label  = data.next_batch()</div><div class="line">        sess.run(train_step,feed_dict=&#123;x: batch_x, label: batch_label&#125;)</div></pre></td></tr></table></figure>
<h1 id="3、变量共享"><a href="#3、变量共享" class="headerlink" title="3、变量共享"></a>3、变量共享</h1><p>当你在使用Tensorflow时，你想在一个地方初始化所有的变量，比如我想多次实例化我的graph或者我想在GPU集群上训练，我们需要共享变量。有以下两个解决方案：<br>其中一个方法是创建一个map，在需要使用的地方调用key获得value。但缺点是它大破了封装的思想。<br>TensorFlow的variable scope解决了这个问题，它为我们提供了一个提供了一个命名空间，避免了冲突。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">with tf.variable_scope(&quot;hello world&quot;):</div><div class="line">              v=tf.get_variable(&quot;v&quot;,shape=[1])</div><div class="line"># v.name== &quot; hello world/v:0&quot;</div><div class="line">with tf.variable_scope(&quot;hello world&quot; reuse=True):</div><div class="line">              v=tf.get_variable(&quot;v&quot;,shape=[1])</div><div class="line"># 可以找到共享的变量v</div><div class="line">with tf.variable_scope(&quot;hello world&quot; reuse=False):</div><div class="line">              v=tf.get_variable(&quot;v&quot;,shape=[1])</div><div class="line"># CRASH hello world/v:0 已经存在了</div></pre></td></tr></table></figure></p>
<h1 id="4、官方demo-（Official-Demo）："><a href="#4、官方demo-（Official-Demo）：" class="headerlink" title="4、官方demo （Official Demo）："></a>4、官方demo （Official Demo）：</h1><p>MNIST手写体识别</p>
<h2 id="4-1、基于Softmax逻辑回归"><a href="#4-1、基于Softmax逻辑回归" class="headerlink" title="4.1、基于Softmax逻辑回归"></a>4.1、基于Softmax逻辑回归</h2><p>利用softmax regression,训练一个手写体分类：<br><figure class="highlight plain"><figcaption><span>for downloading and reading MNIST data."""</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">from __future__ import absolute_import</div><div class="line">from __future__ import division</div><div class="line">from __future__ import print_function</div><div class="line"></div><div class="line">import gzip</div><div class="line">import os</div><div class="line">import tempfile</div><div class="line"></div><div class="line">import numpy</div><div class="line">from six.moves import urllib</div><div class="line">from six.moves import xrange  # pylint: disable=redefined-builtin</div><div class="line">import tensorflow as tf</div><div class="line">from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets</div><div class="line">from tensorflow.examples.tutorials.mnist import input_data</div><div class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</div><div class="line"># 定义一个placeholder,</div><div class="line">x = tf.placeholder(tf.float32, [None, 784])</div><div class="line">W = tf.Variable(tf.zeros([784, 10]))</div><div class="line">b = tf.Variable(tf.zeros([10]))</div><div class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</div><div class="line">#定义一个用于存储正确标示的占位符</div><div class="line">y_=tf.placeholder(&quot;float&quot;,[None,10])</div><div class="line">#交叉熵损失函数</div><div class="line">cross_entropy=-tf.reduce_sum(y_*tf.log(y))</div><div class="line">#梯度下降进行训练</div><div class="line">train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)</div><div class="line">init=tf.initialize_all_variables()</div><div class="line">sess=tf.Session()</div><div class="line">sess.run(init)</div><div class="line">#随机梯度下降</div><div class="line">for i in range(1000):</div><div class="line">    batch_xs, batch_ys=mnist.train.next_batch(100)</div><div class="line">    print(sess.run(train_step,feed_dict=&#123;x:batch_xs,y_:batch_ys&#125;))</div><div class="line">correct_prediction=tf.equal(tf.argmax(y,1),tf.argmax(y_,1))</div><div class="line">accuracy=tf.reduce_mean(tf.cast(correct_prediction,&quot;float&quot;))</div><div class="line">print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;))</div></pre></td></tr></table></figure></p>
<h2 id="4-2、基于CNN神经网络"><a href="#4-2、基于CNN神经网络" class="headerlink" title="4.2、基于CNN神经网络"></a>4.2、基于CNN神经网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div></pre></td><td class="code"><pre><div class="line"># 利用CNN,训练一个手写体分类</div><div class="line">&quot;&quot;&quot;Functions for downloading and reading MNIST data.&quot;&quot;&quot;</div><div class="line">from __future__ import absolute_import</div><div class="line">from __future__ import division</div><div class="line">from __future__ import print_function</div><div class="line"></div><div class="line">import gzip</div><div class="line">import os</div><div class="line">import tempfile</div><div class="line"></div><div class="line">import numpy</div><div class="line">from six.moves import urllib</div><div class="line">from six.moves import xrange  # pylint: disable=redefined-builtin</div><div class="line">import tensorflow as tf</div><div class="line">from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets</div><div class="line">from tensorflow.examples.tutorials.mnist import input_data</div><div class="line"></div><div class="line"># 将系统默认的session作ion</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)</div><div class="line"># 正确答案</div><div class="line">y_ = tf.placeholder(&apos;float&apos;, [None, 10])</div><div class="line"></div><div class="line"></div><div class="line"># 定义函数用于初始化权值,和bias</div><div class="line">def weight_initialize(shape):</div><div class="line">    # 标准差为一,初始化权值</div><div class="line">    initial = tf.truncated_normal(shape, stddev=0.1)</div><div class="line">    return tf.Variable(initial)</div><div class="line"></div><div class="line"></div><div class="line">def bias_variable(shape):</div><div class="line">    initail = tf.constant(0.1, shape=shape)</div><div class="line">    return tf.Variable(initail)</div><div class="line"></div><div class="line"></div><div class="line">x = tf.placeholder(&apos;float&apos;, [None, 784])</div><div class="line"></div><div class="line"></div><div class="line"># 卷积</div><div class="line">def conv2d(x, W):</div><div class="line">    #tf.nn.conv2d提供了一个非常方便的函数来实现卷积层向前传播的算法,这个函数的第一个输入为</div><div class="line">    #当前层节点矩阵,这个矩阵是一个四维矩阵,后面的三个维度对应一个节点矩阵,第一个维度对应一个</div><div class="line">    #输入batch.比如在输入层,input[0,:,:,:]表示第一张图片,input[1,:,:,:]表示第张图片二,tf.nn.conv2d</div><div class="line">    #第二个参数提供了卷积层的权重,第三个参数为步长,步长第一维,和最后一维固定是1,最后一个参数是填充方法</div><div class="line">    #SAME表示全0填充,VALID表示不添加</div><div class="line">    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)</div><div class="line"></div><div class="line"></div><div class="line"># 池化</div><div class="line">def max_pool_2x2(x):</div><div class="line">    #ksize提供了过滤器的大小为2*2</div><div class="line">    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=&apos;SAME&apos;)</div><div class="line"></div><div class="line"></div><div class="line"># 卷积层第一层</div><div class="line">W_conv1 = weight_initialize([5, 5, 1, 32])</div><div class="line">b_conv1 = bias_variable([32])</div><div class="line">x_image = tf.reshape(x, [-1, 28, 28, 1])</div><div class="line"># 卷积层第一层的relu和池化</div><div class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</div><div class="line">h_pool1 = max_pool_2x2(h_conv1)</div><div class="line"># 5*5*32</div><div class="line"># 卷积层第二层</div><div class="line">W_conv2 = weight_initialize([5, 5, 32, 64])</div><div class="line">b_conv2 = bias_variable([64])</div><div class="line"># 卷积层第二层的relu和池化</div><div class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</div><div class="line">h_pool2 = max_pool_2x2(h_conv2)</div><div class="line"># 全连接层的weight和bias</div><div class="line">W_fc1 = weight_initialize([7 * 7 * 64, 1024])</div><div class="line">b_fc1 = bias_variable([1024])</div><div class="line"># output层</div><div class="line">h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])</div><div class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</div><div class="line"></div><div class="line"># 使用dropout防止过拟合</div><div class="line"># 过拟合概率</div><div class="line">keep_prob = tf.placeholder(&quot;float&quot;)</div><div class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</div><div class="line"># 1024个神经元*10个输出</div><div class="line">W_fc2 = weight_initialize([1024, 10])</div><div class="line">b_fc2 = bias_variable([10])</div><div class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2))</div><div class="line"># 定义一个损失函数</div><div class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))</div><div class="line">train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)</div><div class="line">correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))</div><div class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction,&quot;float&quot;))</div><div class="line"># 初始化所有变量</div><div class="line">sess.run(tf.initialize_all_variables())</div><div class="line">for i in range(2000):</div><div class="line">    batch = mnist.train.next_batch(50)</div><div class="line">    if i % 100 == 0:</div><div class="line">        train_accuracy = accuracy.eval(feed_dict=&#123;</div><div class="line">            x: batch[0], y_: batch[1], keep_prob: 1.0</div><div class="line">        &#125;)</div><div class="line">        print(&quot;step %d, train accuracy %g&quot; % (i, train_accuracy))</div><div class="line">    train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: [0.5]&#125;)</div><div class="line"></div><div class="line">print(&quot;test accuracy %g&quot; % accuracy.eval(feed_dict=&#123;</div><div class="line">    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;))</div></pre></td></tr></table></figure>
<p>训练结果如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/7058214-44a150cc63f80034.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="result.png"></p>
<p>别着急，我还要再废话一下：对于上面的实现，我们发现在网络层数增多的情况下，变量的命名可能会重复，造成错误，为了保证代码的可读性，我们可以采用如下的方式表示一个层的计算：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">#第一个卷积层的参数</div><div class="line">CONV1_DEEP=32</div><div class="line">CONV1_SIZE=5</div><div class="line">NUM_CHANNAL=1</div><div class="line">#第二个卷积层的参数</div><div class="line">CONV2_DEEP=64</div><div class="line">CONV2_SIZE=5</div><div class="line">#全连结层的节点</div><div class="line">FC_SIZE=512</div><div class="line"></div><div class="line">#第一个卷积层的实现</div><div class="line">with tf.name_scope(&quot;Layer1-conv1&quot;):</div><div class="line">    #定义权值,实际weight的名称为Layer1-conv1/weight</div><div class="line">    weight=tf.get_variable(&quot;weight&quot;,[CONV1_SIZE,CONV1_SIZE,NUM_CHANNAL,CONV1_DEEP],</div><div class="line">                           initializer=tf.truncated_normal(stddev=0.1))</div><div class="line">    #定义bias</div><div class="line">    conv1_bias=tf.get_variable(&quot;bias&quot;,[CONV1_DEEP],initializer=tf.constant_initializer(0.1))</div><div class="line">    conv1=tf.conv2d(x, weight, strides=[1, 1, 1, 1], padding=&apos;SAME&apos;)</div><div class="line">    relu1=tf.nn.relu(tf.matmul(x,weight))</div><div class="line"></div><div class="line"></div><div class="line">#第一个卷积层的池化</div><div class="line">with tf.name_scope(&quot;Max-pooling&quot;):</div><div class="line">    pool=tf.nn.max_pool(relu1,ksize=[1,1,1,1],strides=[1,2,2,1],padding=&apos;SAME&apos;)</div><div class="line"></div><div class="line"></div><div class="line">#如果在构造大型网络的过程中，如果一个层用6行代码，还是太臃肿，这时候可以使用更简洁的</div><div class="line">#TensorFlow-Slim工具来处理</div><div class="line">net=slim.conv2d(input,32,[3,3])</div></pre></td></tr></table></figure></p>
<h1 id="5、总结："><a href="#5、总结：" class="headerlink" title="5、总结："></a>5、总结：</h1><ul>
<li>TensorFlow 每个结点都对应着梯度操作</li>
<li>Computational graphy 很容易backwards每一个结点。</li>
<li>这些都是自动完成的。</li>
<li>TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorflow不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/05/tensor/" data-id="cj8bs1ip600038w1nadq8as6e" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/09/03/distributed/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          分布式资源管理
        
      </div>
    </a>
  
  
    <a href="/2017/08/03/callback/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">JAVA回调机制(CallBack)详解</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">September 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/09/03/distributed/">分布式资源管理</a>
          </li>
        
          <li>
            <a href="/2017/08/05/tensor/">TensorFlow Tutorial-1</a>
          </li>
        
          <li>
            <a href="/2017/08/03/callback/">JAVA回调机制(CallBack)详解</a>
          </li>
        
          <li>
            <a href="/2017/08/02/class/">ClassLoader和类加载机制</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Jason Chu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>